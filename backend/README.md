# MNIST Backend

Ce service constitue le backend de notre architecture MLOps. Son rÃ´le principal est de servir le modÃ¨le de classification de chiffres MNIST via une API REST.

## RÃ´le dans l'architecture MLOps

Le backend est un composant central qui expose les capacitÃ©s de prÃ©diction du modÃ¨le entraÃ®nÃ©. Il est dÃ©couplÃ© du pipeline d'entraÃ®nement et de l'interface utilisateur, ce qui permet des mises Ã  jour et des montÃ©es en charge indÃ©pendantes.

- **Exposition du modÃ¨le** : Il charge le modÃ¨le entraÃ®nÃ© (`convnet.pt`) et l'expose via un endpoint (par exemple, `/predict`).
- **InfÃ©rence** : Il reÃ§oit des donnÃ©es (images de chiffres), les prÃ©traite si nÃ©cessaire, et retourne les prÃ©dictions du modÃ¨le.
- **DÃ©ploiement continu** : Dans une architecture MLOps de niveau 2, ce service est packagÃ© dans une image Docker et dÃ©ployÃ© automatiquement via notre pipeline CI/CD dÃ¨s qu'un nouveau modÃ¨le est validÃ© et promu.

## Technologies

- **FastAPI** : Pour crÃ©er une API performante et simple Ã  utiliser.
- **PyTorch** : Pour charger et utiliser le modÃ¨le de Deep Learning.
- **Docker** : Pour packager le service et ses dÃ©pendances dans une image portable.

## DÃ©marrage

Pour lancer le service localement (gÃ©nÃ©ralement orchestrÃ© par `docker-compose` depuis le dossier `mnist-deployment`):

```bash
docker build -t mnist-backend .
docker run -p 8000:8000 mnist-backend
```

## ğŸ—ï¸ Architecture

```
ğŸ“¦ mnist-backend/           # ğŸ¯ SERVING UNIQUEMENT
â”œâ”€â”€ src/api/               # API FastAPI
â”œâ”€â”€ src/models/            # DÃ©finitions de modÃ¨les (pas d'entraÃ®nement)
â”œâ”€â”€ models/                # Artefacts de modÃ¨les prÃ©-entraÃ®nÃ©s
â””â”€â”€ tests/                 # Tests de l'API

ğŸ“¦ kedro/                  # ğŸ‹ï¸ ENTRAÃNEMENT UNIQUEMENT  
â”œâ”€â”€ pipelines/             # Pipelines Kedro (data + training)
â”œâ”€â”€ conf/                  # Configuration MLOps
â””â”€â”€ data/                  # DonnÃ©es et artifacts MLflow
```

## ğŸ”„ Workflow de Production

1. **EntraÃ®nement** : `kedro run` (dans le projet kedro)
2. **Artefacts** : ModÃ¨le sauvegardÃ© dans `kedro/data/`
3. **Copie** : ModÃ¨le copiÃ© vers `mnist-backend/models/`
4. **Serving** : API chargÃ©e et prÃªte pour les prÃ©dictions

## ğŸš€ DÃ©marrage rapide

### PrÃ©requis
Assurez-vous qu'un modÃ¨le entraÃ®nÃ© existe dans `models/convnet.pt`. 
Si ce n'est pas le cas, entraÃ®nez d'abord avec Kedro :

```bash
# Dans le projet kedro/
kedro run
```

### Lancement de l'API

```bash
# Installation des dÃ©pendances
pip install -r requirements.txt

# Lancement de l'API
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
```

## ğŸ“š API Endpoints

- `POST /api/v1/predict` - Classification d'image (upload fichier)
- `GET /health` - Statut de l'API  
- `GET /api/info` - Informations sur l'API et le modÃ¨le
- `GET /docs` - Documentation Swagger interactive
- `GET /redoc` - Documentation ReDoc

## ğŸ³ Docker

```bash
# Build
docker build -t mnist-backend .

# Run
docker run -p 8000:8000 mnist-backend

# Avec volume pour modÃ¨les
docker run -p 8000:8000 -v $(pwd)/models:/app/models mnist-backend
```

## ğŸ“ Structure du Projet

```
src/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py          # ğŸš€ API FastAPI
â””â”€â”€ models/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ convnet.py       # ğŸ§  DÃ©finition du modÃ¨le ConvNet
models/
â””â”€â”€ convnet.pt          # ğŸ’¾ ModÃ¨le prÃ©-entraÃ®nÃ© (depuis Kedro)
tests/
â”œâ”€â”€ unit/                # ğŸ§ª Tests unitaires
â”œâ”€â”€ integration/         # ğŸ”— Tests d'intÃ©gration  
â””â”€â”€ model_validation/    # âœ… Validation de modÃ¨le
```

## ğŸ§ª Tests

```bash
# Tests unitaires
pytest tests/unit/

# Tests d'intÃ©gration 
pytest tests/integration/

# Validation de modÃ¨le
pytest tests/model_validation/

# Tous les tests
pytest
```

## ğŸ”§ CI/CD Pipeline

Le workflow GitHub Actions automatise :

1. **Tests** : QualitÃ© de code (Pylint, Flake8)
2. **Build** : Image Docker avec modÃ¨le
3. **Push** : Publication sur GitHub Container Registry
4. **Deploy** : PrÃªt pour dÃ©ploiement via `mnist-deployment`

## ğŸ¤ IntÃ©gration avec Kedro

Pour utiliser un nouveau modÃ¨le entraÃ®nÃ© :

```bash
# 1. EntraÃ®ner avec Kedro
cd ../kedro && kedro run

# 2. Copier le modÃ¨le
cp kedro/data/06_models/convnet.pt mnist-backend/models/

# 3. RedÃ©marrer l'API
# L'API chargera automatiquement le nouveau modÃ¨le
``` 